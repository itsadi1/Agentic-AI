{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsadi1/Agentic-AI/blob/main/Agenticai(ClimateChange).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CNH0Ia309_po"
      },
      "outputs": [],
      "source": [
        "#  !pip install ibm_watsonx_ai\n",
        "#  !pip install langchain_ibm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "57fdf44c-1700-46cd-add3-51a6e4b86a68"
      },
      "outputs": [],
      "source": [
        "from ibm_watsonx_ai import APIClient,Credentials\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
        "from langchain_ibm import WatsonxLLM\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "import textwrap, json, re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "56ff249d-3182-4801-ba1e-351374c3adec"
      },
      "outputs": [],
      "source": [
        "# Initialize credentials and API client\n",
        "credentials = Credentials(\n",
        "    url=userdata.get('ibm_cloud_url'),\n",
        "    api_key=userdata.get('watson_api_key')\n",
        ")\n",
        "project_id = userdata.get('ibm_project_id')\n",
        "client = APIClient(credentials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "947bef67-2638-47cb-9d4c-580788b31b73"
      },
      "outputs": [],
      "source": [
        "api_client = APIClient(credentials=credentials, project_id=project_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8eeb599c-4703-4ea1-907a-f2d98d9dc4c9"
      },
      "outputs": [],
      "source": [
        "parameters = {\n",
        "    # GenParams.DECODING_METHOD: DecodingMethods.GREEDY.value,  # Greedy for deterministic output\n",
        "    GenParams.MAX_NEW_TOKENS: 1000,\n",
        "    GenParams.MIN_NEW_TOKENS: 100,\n",
        "    GenParams.TEMPERATURE: 0.85,\n",
        "    GenParams.TOP_K: 20,\n",
        "    GenParams.TOP_P: 0.6\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "595b235c-23ed-46f2-b098-2f10c25debdc",
        "outputId": "4b3ec29e-132d-4cc6-fa12-4efa5cdf0fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'FLAN_T5_XL': 'google/flan-t5-xl', 'GRANITE_13B_INSTRUCT_V2': 'ibm/granite-13b-instruct-v2', 'GRANITE_3_2_8B_INSTRUCT': 'ibm/granite-3-2-8b-instruct', 'GRANITE_3_2B_INSTRUCT': 'ibm/granite-3-2b-instruct', 'GRANITE_3_3_8B_INSTRUCT': 'ibm/granite-3-3-8b-instruct', 'GRANITE_3_8B_INSTRUCT': 'ibm/granite-3-8b-instruct', 'GRANITE_8B_CODE_INSTRUCT': 'ibm/granite-8b-code-instruct', 'GRANITE_GUARDIAN_3_2B': 'ibm/granite-guardian-3-2b', 'GRANITE_GUARDIAN_3_8B': 'ibm/granite-guardian-3-8b', 'GRANITE_VISION_3_2_2B': 'ibm/granite-vision-3-2-2b', 'LLAMA_2_13B_CHAT': 'meta-llama/llama-2-13b-chat', 'LLAMA_3_2_11B_VISION_INSTRUCT': 'meta-llama/llama-3-2-11b-vision-instruct', 'LLAMA_3_2_1B_INSTRUCT': 'meta-llama/llama-3-2-1b-instruct', 'LLAMA_3_2_3B_INSTRUCT': 'meta-llama/llama-3-2-3b-instruct', 'LLAMA_3_2_90B_VISION_INSTRUCT': 'meta-llama/llama-3-2-90b-vision-instruct', 'LLAMA_3_3_70B_INSTRUCT': 'meta-llama/llama-3-3-70b-instruct', 'LLAMA_3_405B_INSTRUCT': 'meta-llama/llama-3-405b-instruct', 'LLAMA_4_MAVERICK_17B_128E_INSTRUCT_FP8': 'meta-llama/llama-4-maverick-17b-128e-instruct-fp8', 'LLAMA_GUARD_3_11B_VISION': 'meta-llama/llama-guard-3-11b-vision', 'MISTRAL_LARGE': 'mistralai/mistral-large', 'MISTRAL_MEDIUM_2505': 'mistralai/mistral-medium-2505', 'MISTRAL_SMALL_3_1_24B_INSTRUCT_2503': 'mistralai/mistral-small-3-1-24b-instruct-2503', 'PIXTRAL_12B': 'mistralai/pixtral-12b'}\n"
          ]
        }
      ],
      "source": [
        "api_client.foundation_models.TextModels.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fdcd9dd3-dde5-469a-8a14-e3970176b579"
      },
      "outputs": [],
      "source": [
        "# Initialize LLM\n",
        "llm = WatsonxLLM(\n",
        "    model_id=\"meta-llama/llama-4-maverick-17b-128e-instruct-fp8\",\n",
        "    url=credentials[\"url\"],\n",
        "    apikey=credentials[\"api_key\"],\n",
        "    project_id=project_id,\n",
        "    params=parameters\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Wrapping Content\n",
        "def printable(base):\n",
        "  if type(base)==dict:\n",
        "    for role, content in base.items():\n",
        "        content_str = json.dumps(content) if not isinstance(content, str) else content\n",
        "        wrapped_content = textwrap.fill(content_str, width=80, initial_indent=\"  \", subsequent_indent=\"  \")\n",
        "        print(f\"{role.capitalize()} Agent:\\n{wrapped_content}\\n\")\n",
        "  elif type(base)==str:\n",
        "    wrapped_content = textwrap.fill(base, width=80, initial_indent=\"  \", subsequent_indent=\"  \")\n",
        "    print(wrapped_content)\n",
        "  else:\n",
        "    print(\"Error\")"
      ],
      "metadata": {
        "id": "JewD5shT5cxo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "df9f3281-273e-4486-a834-feddd51d7b0e"
      },
      "outputs": [],
      "source": [
        "def process_query(role: str, query: str, llm) -> str:\n",
        "    # Define prompt template for JSON output using 'role' and 'question'\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"role\", \"question\"],\n",
        "        template=\"\"\"\n",
        "Given the role \"{role}\" and the question \"{question}\", provide the answer in JSON format as follows:\n",
        "```json\n",
        "{{\n",
        "    \"role\": \"{role}\",\n",
        "    \"question\": \"{question}\",\n",
        "    \"answer\": \"<your concise answer in one sentence, without restating the question or adding explanations>\",\n",
        "    \"verdict\": \"[negative,neutral,positive]\"\n",
        "}}\n",
        "Just provide the Final Answer directly in JSON format, ensuring the response is valid JSON.\n",
        "\"\"\"\n",
        "    )\n",
        "\n",
        "    # Format the prompt using role and query (question)\n",
        "    formatted_prompt = prompt.format(role=role, question=query)\n",
        "\n",
        "    # Use the LLM to generate the response\n",
        "    raw_response = llm.invoke(formatted_prompt)\n",
        "    st=raw_response.index('{')\n",
        "    en=raw_response.index('}')\n",
        "    return json.loads(raw_response[st:en+1])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Agent: Env\n",
        "class EnvAgent:\n",
        "  def __init__(self,llm):\n",
        "    self.role = \"Act as an environment expert keeping up with the latest climate data from IPCC, NASA, and global monitoring systems.\"\n",
        "    self.task = \"Provide climate data and environmental insights from IPCC, NASA, and global monitoring systems.\"\n",
        "    self.llm = llm\n",
        "  def run(self):\n",
        "    return  process_query(self.role, self.task, self.llm).get(\"answer\")\n",
        "\n",
        "  def query(self, userquery):\n",
        "    return process_query(self.role, userquery, self.llm)\n",
        "\n",
        "  def get_role(self):\n",
        "    return \"env\""
      ],
      "metadata": {
        "id": "CT0pJp5He_kr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent: Tech\n",
        "class TechAgent:\n",
        "    def __init__(self, llm):\n",
        "        self.role = \"Act as a technology and economics expert analyzing the feasibility and costs of climate mitigation technologies.\"\n",
        "        self.task = \"Provide technology and economic feasibility data for climate mitigation solutions.\"\n",
        "        self.llm = llm\n",
        "\n",
        "    def run(self):\n",
        "        return process_query(self.role, self.task, self.llm).get(\"answer\")\n",
        "\n",
        "    def query(self, userquery):\n",
        "        return process_query(self.role, userquery, self.llm)\n",
        "\n",
        "    def get_role(self):\n",
        "        return \"tech\""
      ],
      "metadata": {
        "id": "9e6kBAMbwSja"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent: Social\n",
        "class SocialAgent:\n",
        "    def __init__(self, llm):\n",
        "        self.role = \"Act as a social scientist analyzing public sentiment and social impacts of climate policies using surveys and online data.\"\n",
        "        self.task = \"Provide public sentiment and social impact data for climate policies.\"\n",
        "        self.llm = llm\n",
        "\n",
        "    def run(self):\n",
        "        return process_query(self.role, self.task, self.llm).get(\"answer\")\n",
        "\n",
        "    def query(self, userquery):\n",
        "        return process_query(self.role, userquery, self.llm)\n",
        "\n",
        "    def get_role(self):\n",
        "        return \"social\""
      ],
      "metadata": {
        "id": "2JlVD0Owwnwy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent: Policy\n",
        "class PolicyAgent:\n",
        "    def __init__(self, llm):\n",
        "        self.role = \"Act as a policy analyst reviewing global and national climate policies from UNFCCC, government portals, and NGOs.\"\n",
        "        self.task = \"Provide global and national climate policy data and alignment analysis.\"\n",
        "        self.llm = llm\n",
        "\n",
        "    def run(self):\n",
        "        return process_query(self.role, self.task, self.llm).get(\"answer\")\n",
        "\n",
        "    def query(self, userquery):\n",
        "        return process_query(self.role, userquery, self.llm)\n",
        "\n",
        "    def get_role(self):\n",
        "        return \"policy\""
      ],
      "metadata": {
        "id": "JhLs8QMowrFG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CentralAI:\n",
        "  def __init__(self):\n",
        "    self.role=\"Act as the head of a climate policy department, coordinating and synthesizing insights from technology, social, policy, and environmental teams via attached comprehensive knowledge base to develop evidence-based climate strategies.\"\n",
        "    self.kb = {}\n",
        "    self.tech = TechAgent(llm)\n",
        "    self.policy = PolicyAgent(llm)\n",
        "    self.social = SocialAgent(llm)\n",
        "    self.env = EnvAgent(llm)\n",
        "    self.agents = [self.tech, self.policy, self.social, self.env]\n",
        "\n",
        "  def create_kb(self):\n",
        "    self.kb[\"tech\"] = self.tech.run()\n",
        "    self.kb[\"policy\"] = self.policy.run()\n",
        "    self.kb[\"social\"] = self.social.run()\n",
        "    self.kb[\"env\"] = self.env.run()\n",
        "\n",
        "  def research(self) -> str:\n",
        "    if self.kb == {}:\n",
        "      self.create_kb()\n",
        "    # Define prompt template for JSON output using 'role' and 'question'\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"role\", \"knowledge_base\"],\n",
        "        template=\"\"\"\n",
        "Given the role \"{role}\" and the complete knowledge base \"{knowledge_base}\", provide the answer in JSON format as follows:\n",
        "```json\n",
        "{{\n",
        "    \"prediction\": \"<your concise predicted practice/policy/law in one sentence creating impact towards the goal of climate actions, without restating the question or adding explanations>\",\n",
        "}}\n",
        "Just provide the Final Answer directly in JSON format, ensuring the response is valid JSON.\n",
        "\"\"\"\n",
        "    )\n",
        "\n",
        "    # Format the prompt using role and query (question)\n",
        "    formatted_prompt = prompt.format(role=self.role, knowledge_base=self.kb)\n",
        "\n",
        "    # Use the LLM to generate the response\n",
        "    raw_response = llm.invoke(formatted_prompt)\n",
        "    st=raw_response.index('{')\n",
        "    en=raw_response.index('}')\n",
        "    return json.loads(raw_response[st:en+1])\n",
        "\n",
        "  def process(self, userquery, max_recursions =3):\n",
        "    counter=[]\n",
        "    prefix = \"Based on your role, make an unbaised judgement on the provided statement: \"\n",
        "    for agent in self.agents:\n",
        "      reply = agent.query(prefix+userquery)\n",
        "      if reply.get(\"verdict\") == \"negative\":\n",
        "        self.kb[agent.get_role()] += \"\\n\"+reply.get(\"answer\")\n",
        "        counter.append(False)\n",
        "      else:\n",
        "        counter.append(True)\n",
        "    while not all(counter) and max_recursions > 0:\n",
        "      userquery = self.research().get(\"prediction\")\n",
        "      return self.process(userquery, max_recursions=max_recursions-1)\n",
        "\n",
        "    return userquery, counter"
      ],
      "metadata": {
        "id": "vQZ9XOFBvaeV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ ==\"__main__\":\n",
        "  cai = CentralAI()\n",
        "  initial_prediction = cai.research().get(\"prediction\")\n",
        "  final_prediction,verdicts=cai.process(initial_prediction)\n",
        "  format_verdict = lambda x: \"positive\" if x else \"negative\"\n",
        "  verdict = list(map(format_verdict,verdicts))\n",
        "  print(\"Knowledge Base populated by Multiple Specialized AI Agents: \")\n",
        "  printable(cai.kb)\n",
        "  print(\"Final Prediction:\")\n",
        "  printable(final_prediction)\n",
        "  print(\"Agents Validation:\",verdict)"
      ],
      "metadata": {
        "id": "7RsychBTykpW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4eb538-ba92-4cfb-a872-99ce52773624"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge Base populated by Multiple Specialized AI Agents: \n",
            "Tech Agent:\n",
            "  Renewable energy technologies like solar and wind have become increasingly\n",
            "  cost-competitive with fossil fuels, with levelized costs ranging from $30 to\n",
            "  $50 per MWh.\n",
            "\n",
            "Policy Agent:\n",
            "  Global climate policies, such as the Paris Agreement, are aligned with\n",
            "  national policies, like the US Climate Action Plan, in reducing greenhouse gas\n",
            "  emissions and promoting renewable energy.\n",
            "\n",
            "Social Agent:\n",
            "  Our analysis indicates a generally positive public sentiment towards climate\n",
            "  policies, with 62% of surveyed individuals supporting stricter emission\n",
            "  controls.\n",
            "\n",
            "Env Agent:\n",
            "  The latest climate data indicates a global temperature rise of 1.1°C above\n",
            "  pre-industrial levels, with severe implications for ecosystems and human\n",
            "  societies.\n",
            "\n",
            "Final Prediction:\n",
            "  Implement a global renewable energy mandate with emission reduction targets.\n",
            "Agents Validation: ['positive', 'positive', 'positive', 'positive']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}